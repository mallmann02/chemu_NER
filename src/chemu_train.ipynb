{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you decide to run on Google Colab (uncomment the next line)\n",
    "# !pip install -qU datasets lightning peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import pytorch_lightning as L\n",
    "\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.optimization import get_constant_schedule_with_warmup\n",
    "\n",
    "ACCELERATOR_DEVICE = \"cuda\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stored the ZIP files containing the CHEMU original dataset in my Google Drive, so I use the following code to load my drive and unzip the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd drive/MyDrive/WEG-VENT\n",
    "\n",
    "# !unzip -n ./data/chemu.ee.dev.zip -d ./data\n",
    "# !unzip -n ./data/chemu.ee.train.zip -d ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines let's you access the tensorboard logs during training. The \"logs\" directory is created in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHEMUDataset(Dataset):\n",
    "    label_to_idx = {\n",
    "        \"0\": 0,\n",
    "        \"STARTING_MATERIAL\": 1,\n",
    "        \"REAGENT_CATALYST\": 2,\n",
    "        \"REACTION_PRODUCT\": 3,\n",
    "        \"SOLVENT\": 4,\n",
    "        \"OTHER_COMPOUND\": 5,\n",
    "        \"TIME\": 6,\n",
    "        \"TEMPERATURE\": 7,\n",
    "        \"YIELD_PERCENT\": 8,\n",
    "        \"YIELD_OTHER\": 9,\n",
    "        \"EXAMPLE_LABEL\": 10,\n",
    "        \"REACTION_STEP\": 11,\n",
    "        \"WORKUP\": 12,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer=\"bert-base-uncased\",\n",
    "        max_length=512,\n",
    "        split=\"train\",\n",
    "        data_dir_path=\"../data/ee\",\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.verbose = verbose\n",
    "        if split not in [\"train\", \"dev\"]:\n",
    "            raise ValueError(\"split must be one of 'train', 'dev', or 'test'\")\n",
    "\n",
    "        files = os.listdir(data_dir_path + f\"_{split}\")\n",
    "        files = [f.split(\".\")[0] for f in files if f.endswith(\".txt\")]\n",
    "        self.files = sorted(files)\n",
    "        print(f\"Found {len(self.files)} files.\")\n",
    "\n",
    "        self.tokens = []\n",
    "        self.targets = []\n",
    "        self.attention_masks = []\n",
    "        # self.files = self.files[:1] if split == 'train' else self.files[:50]  # For debugging, use only the first file in dev split\n",
    "        for file in self.files:\n",
    "            # if file not in ['0096', '0130', '0163', '0215', '0225', '0421', '0541', '0544', '0590', '0692', '0696', '0762', '0890', '1069', '1082', '1243', '1278', '1314', '1352', '1388', '1404', '1468']:\n",
    "            try:\n",
    "                file_tokens, file_attention_mask, file_targets = self.load_file(file)\n",
    "                if len(file_tokens) > self.max_length:\n",
    "                    if self.verbose:\n",
    "                        print(\n",
    "                            f\"File {file} exceeds max length ({len(file_tokens)} > {self.max_length})\"\n",
    "                        )\n",
    "                self.tokens.append(file_tokens)\n",
    "                self.targets.append(file_targets)\n",
    "                self.attention_masks.append(file_attention_mask)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file}: {e}\")\n",
    "                continue\n",
    "\n",
    "    def load_file(self, file):\n",
    "        with open(f\"../data/ee_{self.split}/{file}.txt\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        with open(f\"../data/ee_{self.split}/{file}.ann\") as f:\n",
    "            annotations = f.readlines()\n",
    "\n",
    "        ner_ann, ee_ann = self.generate_dataframe(annotations)\n",
    "        tokens, targets = self.structure_data(text, ner_ann, ee_ann)\n",
    "        attention_mask = torch.ones(len(tokens), dtype=torch.int)\n",
    "        # tokens, targets = torch.tensor(tokens, dtype=torch.long), torch.tensor(targets, dtype=torch.int)\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        # print(targets)\n",
    "        # print(torch.tensor(targets))\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        return tokens, attention_mask, targets\n",
    "\n",
    "    def generate_dataframe(self, annotations):\n",
    "        ner_ann = []\n",
    "        ee_ann = []\n",
    "        for line in annotations:\n",
    "            if line.startswith(\"T\"):\n",
    "                # ner_ann.append(line.split('\\t'))\n",
    "                # ner_ann = ner_ann.append({\n",
    "                ner_ann.append(\n",
    "                    {\n",
    "                        \"id\": line.split(\"\\t\")[0],\n",
    "                        \"label\": line.split(\"\\t\")[1].split(\" \")[0],\n",
    "                        \"start\": int(line.split(\"\\t\")[1].split(\" \")[1]),\n",
    "                        \"end\": int(line.split(\"\\t\")[1].split(\" \")[2]),\n",
    "                        \"text\": line.split(\"\\t\")[2].strip(),\n",
    "                    }\n",
    "                )\n",
    "                # }, ignore_index=True)\n",
    "            elif line.startswith(\"R\"):\n",
    "                # ee_ann.append(line.split('\\t'))\n",
    "                ee_ann.append(\n",
    "                    {\n",
    "                        \"id\": line.split(\"\\t\")[0],\n",
    "                        \"Arg1\": line.split(\"\\t\")[1].split(\" \")[1].split(\":\")[1],\n",
    "                        \"Arg2\": line.split(\"\\t\")[1]\n",
    "                        .split(\" \")[2]\n",
    "                        .split(\":\")[1]\n",
    "                        .replace(\"\\n\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        ner_ann = pd.DataFrame(ner_ann)\n",
    "        ee_ann = pd.DataFrame(ee_ann)\n",
    "        ner_ann[\"label\"] = ner_ann[\"label\"].map(self.label_to_idx)\n",
    "\n",
    "        ner_ann = ner_ann.sort_values(by=[\"start\"]).reset_index(drop=True)\n",
    "        if (ner_ann[\"start\"][1:].to_numpy() < ner_ann[\"end\"][:-1].to_numpy()).any():\n",
    "            raise ValueError(\"Overlapping entities found in the annotations.\")\n",
    "\n",
    "        return ner_ann, ee_ann\n",
    "\n",
    "    def structure_data(self, text, ner_ann, ee_ann):\n",
    "        ends = ner_ann[\"end\"].to_numpy()\n",
    "        starts = ner_ann[\"start\"].to_numpy()\n",
    "        labels = ner_ann[\"label\"].to_numpy()\n",
    "\n",
    "        bos_token, eos_token = self.tokenizer(\"\").input_ids\n",
    "        sections = [\n",
    "            [bos_token],\n",
    "            self.tokenizer(text[: starts[0]], add_special_tokens=False).input_ids,\n",
    "        ]\n",
    "        section_labels = [[-1], [0] * len(sections[1])]\n",
    "        for i in range(len(ner_ann)):\n",
    "            sections.append(\n",
    "                self.tokenizer(\n",
    "                    text[starts[i] : ends[i]], add_special_tokens=False\n",
    "                ).input_ids\n",
    "            )\n",
    "            section_labels.append([labels[i]] * len(sections[-1]))\n",
    "            if i + 1 < len(ner_ann):\n",
    "                sections.append(\n",
    "                    self.tokenizer(\n",
    "                        text[ends[i] : starts[i + 1]], add_special_tokens=False\n",
    "                    ).input_ids\n",
    "                )\n",
    "                section_labels.append([0] * len(sections[-1]))\n",
    "        sections.append(\n",
    "            self.tokenizer(text[ends[-1] :], add_special_tokens=False).input_ids\n",
    "        )\n",
    "        sections.append([eos_token])\n",
    "\n",
    "        section_labels.append([0] * len(sections[-2]))\n",
    "        section_labels.append([-1])\n",
    "\n",
    "        tokens = [item for sublist in sections for item in sublist]\n",
    "        targets = [item for sublist in section_labels for item in sublist]\n",
    "\n",
    "        return tokens, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.tokens[idx], self.targets[idx]\n",
    "        return self.tokens[idx], self.attention_masks[idx], self.targets[idx]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        tokens, attention_masks, targets = zip(*batch)\n",
    "        tokens = torch.nn.utils.rnn.pad_sequence(\n",
    "            tokens, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        targets = torch.nn.utils.rnn.pad_sequence(\n",
    "            targets, batch_first=True, padding_value=-1\n",
    "        )\n",
    "        attention_masks = torch.nn.utils.rnn.pad_sequence(\n",
    "            attention_masks, batch_first=True, padding_value=0\n",
    "        )\n",
    "\n",
    "        tokens = tokens[:, : self.max_length]\n",
    "        attention_masks = attention_masks[:, : self.max_length]\n",
    "        targets = targets[:, : self.max_length]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokens,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"labels\": targets,\n",
    "            # 'return_loss': True,\n",
    "        }\n",
    "\n",
    "    def get_class_weights(self):\n",
    "        all_targets = torch.concatenate(self.targets)\n",
    "        all_targets = all_targets[all_targets != -1]\n",
    "        class_counts = torch.bincount(all_targets)\n",
    "        class_weights = 1.0 / class_counts\n",
    "        class_weights = class_weights / class_weights.mean()\n",
    "        return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHEMUDataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: str = \"bert-base-uncased\",\n",
    "        max_length: int = 512,\n",
    "        batch_size: int = 8,\n",
    "        num_workers: int = 4,\n",
    "        data_dir: str = \"./data\",\n",
    "        pin_memory: bool = True,\n",
    "        drop_last: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.data_dir = data_dir\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Tokenizer download (if necessary)\n",
    "        AutoTokenizer.from_pretrained(self.tokenizer)\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = CHEMUDataset(\n",
    "                tokenizer=self.tokenizer, max_length=self.max_length, split=\"train\"\n",
    "            )\n",
    "            self.val_dataset = CHEMUDataset(\n",
    "                tokenizer=self.tokenizer, max_length=self.max_length, split=\"dev\"\n",
    "            )\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = CHEMUDataset(\n",
    "                tokenizer=self.tokenizer,\n",
    "                max_length=self.max_length,\n",
    "                split=\"dev\",  # or 'test' if available\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.train_dataset.collate_fn,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=self.drop_last,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.val_dataset.collate_fn,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.test_dataset.collate_fn,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def get_class_weights(self):\n",
    "        # Only available after setup has run\n",
    "        if self.train_dataset:\n",
    "            return self.train_dataset.get_class_weights()\n",
    "        else:\n",
    "            raise RuntimeError(\"Call setup() before accessing class weights.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class V1Model(nn.Module):\n",
    "    def __init__(self, base_model: str, num_classes: int, lora_config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(ACCELERATOR_DEVICE)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.embedding_model = AutoModel.from_pretrained(base_model)\n",
    "\n",
    "        self.classifier_head = nn.Linear(768, num_classes, bias=False)\n",
    "        nn.init.normal_(self.classifier_head.weight, mean=0.0, std=0.0)\n",
    "        self.classifier_head = self.classifier_head\n",
    "\n",
    "        self.final_layer_activation = nn.Softmax(dim=-1)\n",
    "\n",
    "        if lora_config:\n",
    "            self.embedding_model = get_peft_model(self.embedding_model, lora_config)\n",
    "\n",
    "    def forward(self, text_input_ids, attn_mask):\n",
    "        text_embeddings = self.embedding_model(\n",
    "            text_input_ids, attn_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "        logits = self.classifier_head(text_embeddings)\n",
    "        probas = self.final_layer_activation(logits)\n",
    "\n",
    "        return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Optim (Double scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleScheduler(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: V1Model,\n",
    "        len_train_dataloader: int,\n",
    "        batch_size: int = 16,\n",
    "        head_lr: float = 1e-3,\n",
    "        checkpoint_lr: float = 2e-5,\n",
    "        class_weights: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.len_dataset = len_train_dataloader\n",
    "        self.batch_size = batch_size\n",
    "        self.head_lr = head_lr\n",
    "        self.checkpoint_lr = checkpoint_lr\n",
    "        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-1, weight=class_weights)\n",
    "        self.automatic_optimization = False  # Enable manual optimization\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text_input_ids, label, attention_mask = (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"labels\"],\n",
    "            batch[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        # return from model has shape (batch_size, seq_length, num_classes)\n",
    "        # labels has shape (batch_size, seq_length)\n",
    "        probas = self.model(text_input_ids, attention_mask)\n",
    "\n",
    "        # self.loss_fct.weight = class_weights\n",
    "        loss = self.loss_fct(probas.view(-1, self.model.num_classes), label.view(-1))\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Manually perform optimization\n",
    "        head_optimizer, checkpoint_optimizer = self.optimizers()\n",
    "        head_optimizer.zero_grad()\n",
    "        checkpoint_optimizer.zero_grad()\n",
    "        self.manual_backward(loss)\n",
    "        head_optimizer.step()\n",
    "        checkpoint_optimizer.step()\n",
    "\n",
    "        # Step the schedulers\n",
    "        head_scheduler, checkpoint_scheduler = self.lr_schedulers()\n",
    "        head_scheduler.step()\n",
    "        checkpoint_scheduler.step()\n",
    "\n",
    "        # Log learning rates\n",
    "        self.log(\n",
    "            \"checkpoint_lr\",\n",
    "            checkpoint_optimizer.param_groups[0][\"lr\"],\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"head_lr\", head_optimizer.param_groups[0][\"lr\"], prog_bar=True, on_step=True\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text_input_ids, label, attention_mask = (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"labels\"],\n",
    "            batch[\"attention_mask\"],\n",
    "        )\n",
    "        probas = self.model(text_input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fct(probas.view(-1, self.model.num_classes), label.view(-1))\n",
    "\n",
    "        # Get predictions and flatten for metric calculation\n",
    "        preds = torch.argmax(probas, dim=-1).view(-1).cpu().numpy()\n",
    "        targets = label.view(-1).cpu().numpy()\n",
    "\n",
    "        # Mask out ignored index (-1) for metrics\n",
    "        mask = targets != -1\n",
    "        preds = preds[mask]\n",
    "        targets = targets[mask]\n",
    "\n",
    "        # Compute metrics only if there are valid tokens\n",
    "        if len(targets) > 0:\n",
    "            f1 = f1_score(targets, preds, average=\"macro\", zero_division=0)\n",
    "            precision = precision_score(\n",
    "                targets, preds, average=\"macro\", zero_division=0\n",
    "            )\n",
    "            recall = recall_score(targets, preds, average=\"macro\", zero_division=0)\n",
    "        else:\n",
    "            f1 = precision = recall = 0.0\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_f1\", f1, prog_bar=True)\n",
    "        self.log(\"val_precision\", precision, prog_bar=True)\n",
    "        self.log(\"val_recall\", recall, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define the optimizers\n",
    "        head_optimizer = optim.RAdam(\n",
    "            self.model.classifier_head.parameters(), lr=self.head_lr\n",
    "        )\n",
    "        checkpoint_optimizer = optim.RAdam(\n",
    "            self.model.embedding_model.parameters(), lr=self.checkpoint_lr\n",
    "        )\n",
    "\n",
    "        # Define the schedulers\n",
    "        head_scheduler = get_constant_schedule_with_warmup(\n",
    "            head_optimizer, num_warmup_steps=0\n",
    "        )\n",
    "        checkpoint_scheduler = get_constant_schedule_with_warmup(\n",
    "            checkpoint_optimizer, num_warmup_steps=0,\n",
    "        )\n",
    "\n",
    "        # Return the optimizers and schedulers\n",
    "        return [head_optimizer, checkpoint_optimizer], [\n",
    "            head_scheduler,\n",
    "            checkpoint_scheduler,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Optim (single Scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleScheduler(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: V1Model,\n",
    "        len_train_dataloader: int,\n",
    "        batch_size: int = 16,\n",
    "        head_lr: float = 1e-3,\n",
    "        checkpoint_lr: float = 2e-5,\n",
    "        class_weights: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.len_dataset = len_train_dataloader\n",
    "        self.batch_size = batch_size\n",
    "        self.head_lr = head_lr\n",
    "        self.checkpoint_lr = checkpoint_lr\n",
    "        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-1, weight=class_weights)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text_input_ids, label, attention_mask = (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"labels\"],\n",
    "            batch[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        probas = self.model(text_input_ids, attention_mask)\n",
    "\n",
    "        loss = self.loss_fct(probas.view(-1, self.model.num_classes), label.view(-1))\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\n",
    "            \"lr\",\n",
    "            self.trainer.optimizers[0].param_groups[0][\"lr\"],\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text_input_ids, label, attention_mask = (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"labels\"],\n",
    "            batch[\"attention_mask\"],\n",
    "        )\n",
    "        probas = self.model(text_input_ids, attention_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fct(probas.view(-1, self.model.num_classes), label.view(-1))\n",
    "\n",
    "        # Get predictions and flatten for metric calculation\n",
    "        preds = torch.argmax(probas, dim=-1).view(-1).cpu().numpy()\n",
    "        targets = label.view(-1).cpu().numpy()\n",
    "\n",
    "        # Mask out ignored index (-100) for metrics\n",
    "        mask = targets != -1\n",
    "        preds = preds[mask]\n",
    "        targets = targets[mask]\n",
    "\n",
    "        # Compute metrics only if there are valid tokens\n",
    "        if len(targets) > 0:\n",
    "            f1 = f1_score(targets, preds, average=\"macro\", zero_division=0)\n",
    "            precision = precision_score(\n",
    "                targets, preds, average=\"macro\", zero_division=0\n",
    "            )\n",
    "            recall = recall_score(targets, preds, average=\"macro\", zero_division=0)\n",
    "        else:\n",
    "            f1 = precision = recall = 0.0\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_f1\", f1, prog_bar=True)\n",
    "        self.log(\"val_precision\", precision, prog_bar=True)\n",
    "        self.log(\"val_recall\", recall, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.RAdam(self.model.parameters(), lr=self.checkpoint_lr)\n",
    "        # scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=0.2*(self.len_dataset//self.batch_size))\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Setup & Train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = CHEMUDataModule(\n",
    "    tokenizer=\"bert-base-uncased\",\n",
    "    max_length=512,\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = V1Model(\n",
    "    base_model=\"bert-base-uncased\",\n",
    "    num_classes=len(CHEMUDataset.label_to_idx),\n",
    "    lora_config=LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# lightning = SingleScheduler(\n",
    "#     model=model,\n",
    "#     len_train_dataloader=len(data_module.train_dataloader()),\n",
    "#     batch_size=data_module.batch_size,\n",
    "#     head_lr=5e-4,\n",
    "#     checkpoint_lr=5e-4,\n",
    "#     class_weights=class_weights\n",
    "# )\n",
    "\n",
    "double_lightning = DoubleScheduler(\n",
    "    model=model,\n",
    "    len_train_dataloader=len(data_module.train_dataloader()),\n",
    "    batch_size=data_module.batch_size,\n",
    "    head_lr=1e-3,\n",
    "    checkpoint_lr=2e-4,\n",
    "    class_weights=data_module.get_class_weights()\n",
    ")\n",
    "\n",
    "\n",
    "def get_logger_name():\n",
    "    return (\n",
    "        f\"chemud-{model.__class__.__name__}\"\n",
    "        f\"-{data_module.tokenizer}\"\n",
    "        f\"-clr{double_lightning.checkpoint_lr}\"\n",
    "        f\"-hlr{double_lightning.head_lr}\"\n",
    "        f\"-bs{data_module.batch_size}\"\n",
    "        f\"-alpha{model.embedding_model.peft_config['default'].lora_alpha}\"\n",
    "        f\"-r{model.embedding_model.peft_config['default'].r}\"\n",
    "        f\"-num_classes{len(CHEMUDataset.label_to_idx)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=ACCELERATOR_DEVICE,\n",
    "    devices=1,\n",
    "    logger=L.loggers.TensorBoardLogger(\"logs/\", name=get_logger_name()),\n",
    "    # callbacks=[\n",
    "    #     L.callbacks.ModelCheckpoint(\n",
    "    #         dirpath=\"checkpoints/\",\n",
    "    #         filename=\"chemud-{epoch:02d}-{val_loss:.2f}\",\n",
    "    #         monitor=\"val_loss\",\n",
    "    #         mode=\"min\",\n",
    "    #         save_top_k=1,\n",
    "    #     ),\n",
    "    # ],\n",
    ")\n",
    "\n",
    "# trainer.fit(lightning, datamodule=data_module)\n",
    "trainer.fit(double_lightning, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sent_emb_ready",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
